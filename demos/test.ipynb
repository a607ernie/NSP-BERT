{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# __author__ = \"Sponge_sy\"\n",
    "# Date: 2021/9/11\n",
    "\n",
    "\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"Data Generator\"\"\"\n",
    "\n",
    "    def __init__(self, pattern=\"\", is_pre=True, *args, **kwargs):\n",
    "        super(data_generator, self).__init__(*args, **kwargs)\n",
    "        self.pattern = pattern\n",
    "        self.is_pre = is_pre\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_output_ids = [], [], []\n",
    "        for is_end, text in self.sample(random):\n",
    "            if (self.is_pre):\n",
    "                token_ids, segment_ids = tokenizer.encode(first_text=self.pattern, second_text=text, maxlen=maxlen)\n",
    "            else:\n",
    "                token_ids, segment_ids = tokenizer.encode(first_text=text, second_text=self.pattern, maxlen=maxlen)\n",
    "            source_ids, target_ids = token_ids[:], token_ids[:]\n",
    "            batch_token_ids.append(source_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                yield [batch_token_ids, batch_segment_ids], None\n",
    "                batch_token_ids, batch_segment_ids, = [], []\n",
    "\n",
    "def predict(data_generator_list, data):\n",
    "    print(\"\\n*******************Start to Zero-Shot predict*******************\", flush=True)\n",
    "    patterns_logits = [[] for _ in patterns]\n",
    "    samples_logits = [[] for _ in data]\n",
    "    for i in range(len(data_generator_list)):\n",
    "        print(\"\\nPattern{}\".format(i), flush=True)\n",
    "        data_generator = data_generator_list[i]\n",
    "        counter = 0\n",
    "        for (x, _) in tqdm(data_generator):\n",
    "            outputs = model.predict(x[:2])\n",
    "            print(outputs)\n",
    "            for out in outputs:\n",
    "                logit_pos = out[0].T\n",
    "                patterns_logits[i].append(logit_pos)\n",
    "                samples_logits[counter].append(logit_pos)\n",
    "                counter += 1\n",
    "    preds = []\n",
    "    for i in range(len(patterns_logits[0])):\n",
    "        pred = numpy.argmax([logits[i] for logits in patterns_logits])\n",
    "        print(\"max prob for pattern : \"+str(pred))\n",
    "        preds.append(int(pred))\n",
    "    return preds, samples_logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************Start to Zero-Shot predict*******************\n",
      "\n",
      "Pattern0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00163957 0.99836046]]\n",
      "\n",
      "Pattern1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9938851  0.00611484]]\n",
      "\n",
      "Pattern2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00195668 0.9980433 ]]\n",
      "\n",
      "Pattern3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.247987e-04 9.990752e-01]]\n",
      "\n",
      "Pattern4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.573920e-04 9.993426e-01]]\n",
      "\n",
      "Pattern5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00639352 0.99360645]]\n",
      "max prob for pattern : 1\n",
      "Sample 0:\n",
      "Original Text: 義大利隊贏了歐洲杯\n",
      "Predict label: 体育\n",
      "Logits: [0.0016395687, 0.9938851, 0.001956677, 0.0009247987, 0.000657392, 0.006393523]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load the hyper-parameters-----------------------------------------------------------\n",
    "    maxlen = 128  # The max length 128 is used in our paper\n",
    "    batch_size = 40  # Will not influence the results\n",
    "\n",
    "    # Choose a model----------------------------------------------------------------------\n",
    "    # Recommend to use 'uer-mixed-bert-base'\n",
    "    # model_names = ['google-bert', 'google-bert-small', 'google-bert-zh',\n",
    "    #                'hfl-bert-wwm', 'hfl-bert-wwm-ext',\n",
    "    #                'uer-mixed-bert-tiny', 'uer-mixed-bert-small',\n",
    "    #                'uer-mixed-bert-base', 'uer-mixed-bert-large']\n",
    "    model_name = 'google-bert-zh'\n",
    "\n",
    "    # Choose a dataset----------------------------------------------------------------------\n",
    "    # dataset_names = ['eprstmt', 'tnews', 'csldcp', 'iflytek']\n",
    "    # dataset_name = 'eprstmt'\n",
    "\n",
    "    # Load model and dataset class\n",
    "    bert_model = Model(model_name=model_name)\n",
    "\n",
    "    # Create a template --------------------------------------------------------------------\n",
    "    label_names = ['娱乐', '体育', '音乐', '电竞', '经济', '教育']\n",
    "    patterns = [\"这是一篇{}新闻\".format(label) for label in label_names]\n",
    "    # label_names = ['有精神病','沒病','有憂鬱症','有亞斯柏格症','有焦慮症']\n",
    "    #patterns = [\"這是一句{}的句子\".format(label) for label in label_names]\n",
    "    # patterns = [\"這是一句{}的句子。重鬱症、憂鬱症、亞斯柏格症、焦慮症都是精神病\".format(label) for label in label_names]\n",
    "    # patterns = [\"我是不是{}。重鬱症、憂鬱症、亞斯柏格症、焦慮症都是精神病\".format(label) for label in label_names]\n",
    "    \n",
    "    # Prefix or Suffix-------------------------------------------------------------------\n",
    "    is_pre = True\n",
    "\n",
    "    # Load the demo set--------------------------------------------------------------------\n",
    "    # demo_data_zh = ['梅西超越贝利成为南美射手王',\n",
    "    #              '贾斯汀比伯发布新单曲',\n",
    "    #              '比心APP被下架并永久关闭陪玩功能',\n",
    "    #              '徐莉佳的伦敦奥运金牌氧化了',\n",
    "    #              '10元芯片卖400元!芯片经销商被罚',\n",
    "    #              '北京首批校外培训机构白名单公布',\n",
    "    #              '打藍球很好玩']\n",
    "    \n",
    "    demo_data_zh = ['義大利隊贏了歐洲杯']\n",
    "\n",
    "    # demo_data_zh = ['結果有亞斯柏格症的人竟然是我', # ==== pos =====\n",
    "    #              '自從憂鬱症過後不知道是疾病還是藥物的因素',\n",
    "    #              '憂鬱症過後不知道是疾病還是藥物的因素',\n",
    "    #              '憂鬱症治療超過一年',\n",
    "    #              '精神科醫師判定我為無精神症的重鬱症',\n",
    "    #              '我覺得我有社交焦慮症',\n",
    "    #              '醫生說我只是輕微憂鬱症',\n",
    "    #              '我被判定這種病很久了之前還因為嚴重解離現象住院過',\n",
    "    #              '最後醫生開藥時問我讀什麼的',  # ===== neg =====\n",
    "    #              '上一堆奇怪的心靈課程還硬拉小孩要學',\n",
    "    #              '可是再唸下去我不開心啊焦慮症會發作',\n",
    "    #              '竟然被說是無法讀懂肢體語言和表情',\n",
    "    #              '我也希望我能爬得起床啊我鬧鐘已經設了十幾個',\n",
    "    #              '知道爸爸壽命竟然不是從算命師說出來',\n",
    "    #              '就連手機看詩詞都要看個三次才能看完',\n",
    "    #              '有六個月每天晚上都躲在棉被裡哭'\n",
    "    #              ]\n",
    "\n",
    "\n",
    "    demo_data = demo_data_zh\n",
    "    demo_generator_list = []\n",
    "    for p in patterns:\n",
    "        demo_generator_list.append(data_generator(pattern=p, is_pre=is_pre, data=demo_data, batch_size=batch_size))\n",
    "\n",
    "    \n",
    "    # Build BERT model---------------------------------------------------------------------\n",
    "    tokenizer = Tokenizer('.' + bert_model.dict_path, do_lower_case=True)\n",
    "    # Load BERET model with NSP head\n",
    "    model = build_transformer_model(\n",
    "        config_path='.' + bert_model.config_path, checkpoint_path='.' + bert_model.checkpoint_path, with_nsp=True,\n",
    "    )\n",
    "\n",
    "    # Zero-Shot predict and evaluate-------------------------------------------------------\n",
    "    preds, samples_logits = predict(demo_generator_list, demo_data)\n",
    "    for i, (p, d) in enumerate(zip(preds, demo_data)):\n",
    "        pred_label = label_names[p]\n",
    "        print(\"Sample {}:\".format(i))\n",
    "        print(\"Original Text: {}\".format(d))\n",
    "        print(\"Predict label: {}\".format(pred_label))\n",
    "        print(\"Logits: {}\".format(samples_logits[i]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsp_bert4py3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
